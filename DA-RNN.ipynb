{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal Python packages\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import typing\n",
    "from typing import Tuple\n",
    "\n",
    "# External Python packages\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Module files\n",
    "import utils\n",
    "from cosine_scheduler import CosineLRWithRestarts\n",
    "from adamw import AdamW\n",
    "from modules import Encoder, Decoder\n",
    "from custom_types import DaRnnNet, TrainData, TrainConfig\n",
    "from utils import numpy_to_tvar\n",
    "from constants import device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.setup_log()\n",
    "logger.info(f\"Using computation device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, features, targets, window, time_start, time_end):\n",
    "        self.window = window\n",
    "        self.feats = len(features)\n",
    "        \n",
    "        df_model = df[features + targets].copy()\n",
    "        \n",
    "        # replace outliers with 0\n",
    "        df_model[(np.abs(stats.zscore(df_model)) < 3).all(axis=1)] = 0\n",
    "        df_model = df_model.fillna(0)\n",
    "        index = df_model.loc[(df_model.index>time_start) & (df_model.index<time_end)].index\n",
    "            \n",
    "        df_x = df_model[features]\n",
    "        df_y = df_model[targets]\n",
    "\n",
    "        # Initialize scalers\n",
    "        self.x_scaler = StandardScaler().fit(df_x.values)\n",
    "        self.y_scaler = StandardScaler().fit(df_y.values)\n",
    "        \n",
    "        x_scaled = self.x_scaler.transform(df_x.values)\n",
    "        x_scaled = pd.DataFrame(x_scaled, index=df_x.index)\n",
    "\n",
    "        y_scaled = self.y_scaler.transform(df_y.values)\n",
    "        y_scaled = pd.DataFrame(y_scaled, index=df_y.index)\n",
    "        \n",
    "        # filter df by start and end times\n",
    "        train_X = x_scaled.loc[(x_scaled.index>time_start) & (x_scaled.index<time_end)].values\n",
    "        train_y = y_scaled.loc[(y_scaled.index>time_start) & (y_scaled.index<time_end)].values\n",
    "        \n",
    "        self.data = TrainData(index, train_X, train_y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window]\n",
    "        # If there is a NaN in the sample, just return zeros\n",
    "        if np.isnan(x).any():\n",
    "            return torch.zeros(self.window, self.feats)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.feats.shape[0] - self.window\n",
    "    \n",
    "    \n",
    "    def create_sample(self):\n",
    "        idx = np.random.randint(self.data.feats.shape[0] - self.window)\n",
    "        x = self.data.feats[idx:idx+self.window, :]\n",
    "        y_history = self.data.targs[idx:idx+self.window-1, :]\n",
    "        y_target = np.squeeze(self.data.targs[idx+self.window-1, :])\n",
    "        return (x, y_history, y_target)\n",
    "    \n",
    "    \n",
    "    def generate_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generator function for creating random batches of training data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Infinite loop\n",
    "        while True:\n",
    "            # Allocate a new array for the batch of input signals\n",
    "            x_batch = np.zeros((batch_size, self.window, self.data.feats.shape[1]))\n",
    "\n",
    "            # Allocate a new array for the batch of output signals\n",
    "            y_history_batch = np.zeros((batch_size, self.window - 1, self.data.targs.shape[1]))\n",
    "            y_target_batch = np.zeros((batch_size, self.data.targs.shape[1]))\n",
    "\n",
    "            # Fill the batch with random sequences of data\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                # If the sequence contain any NaN, resample it\n",
    "                contains_nan = True\n",
    "                while contains_nan:\n",
    "                    sample = self.create_sample()\n",
    "                    x_batch[i, :, :] = sample[0]\n",
    "                    y_history_batch[i, :, :] = sample[1]\n",
    "                    y_target_batch[i, :] = sample[2]\n",
    "                    contains_nan = np.isnan(sample[0]).any() or np.isnan(sample[1]).any() or np.isnan(sample[2]).any()\n",
    "\n",
    "            yield (x_batch, y_history_batch, y_target_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def da_rnn(train_data: TrainData, encoder_hidden_size: int, decoder_hidden_size: int, T: int, batch_size: int):\n",
    "\n",
    "    n_targs = train_data.data.targs.shape[1]\n",
    "    train_cfg = TrainConfig(T, int(len(train_data) * 0.7), batch_size, nn.MSELoss())\n",
    "    logger.info(f\"Training size: {train_cfg.train_size:d}.\")\n",
    "\n",
    "    enc_kwargs = {\"input_size\": train_data.data.feats.shape[1], \"hidden_size\": encoder_hidden_size, \"T\": T}\n",
    "    encoder = Encoder(**enc_kwargs).to(device)\n",
    "    \n",
    "    with open(os.path.join(\"data\", \"enc_kwargs.json\"), \"w\") as fi:\n",
    "        json.dump(enc_kwargs, fi, indent=4)\n",
    "\n",
    "    dec_kwargs = {\"encoder_hidden_size\": encoder_hidden_size,\n",
    "                  \"decoder_hidden_size\": decoder_hidden_size, \"T\": T, \"out_feats\": n_targs}\n",
    "    decoder = Decoder(**dec_kwargs).to(device)\n",
    "    \n",
    "    with open(os.path.join(\"data\", \"dec_kwargs.json\"), \"w\") as fi:\n",
    "        json.dump(dec_kwargs, fi, indent=4)\n",
    "\n",
    "    # Standard Adam\n",
    "    #encoder_optimizer = optim.Adam(params=[p for p in encoder.parameters() if p.requires_grad], lr=learning_rate)\n",
    "    #decoder_optimizer = optim.Adam(params=[p for p in decoder.parameters() if p.requires_grad], lr=learning_rate)\n",
    "    \n",
    "    # Adam with weight decay\n",
    "    encoder_optimizer = AdamW(params=[p for p in encoder.parameters() if p.requires_grad], lr=1e-3, weight_decay=1e-4)\n",
    "    decoder_optimizer = AdamW(params=[p for p in decoder.parameters() if p.requires_grad], lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    ## Only Cosine Annealing here\n",
    "    epoch_size = train_cfg.train_size / batch_size\n",
    "    encoder_lr_scheduler = CosineLRWithRestarts(encoder_optimizer, batch_size, epoch_size, restart_period=5, t_mult=1.2)\n",
    "    decoder_lr_scheduler = CosineLRWithRestarts(decoder_optimizer, batch_size, epoch_size, restart_period=5, t_mult=1.2)\n",
    "    \n",
    "    #encoder_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, patience=2)\n",
    "    #decoder_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, patience=2)\n",
    "    \n",
    "    da_rnn_net = DaRnnNet(encoder, decoder, encoder_optimizer, decoder_optimizer, encoder_lr_scheduler, decoder_lr_scheduler)\n",
    "\n",
    "    return train_cfg, da_rnn_net\n",
    "\n",
    "\n",
    "def train(net: DaRnnNet, train_data: TrainData, t_cfg: TrainConfig, n_epochs=10, save_plots=False):\n",
    "    iter_per_epoch = int(np.ceil(t_cfg.train_size * 1. / t_cfg.batch_size))\n",
    "    iter_losses = np.zeros(n_epochs * iter_per_epoch)\n",
    "    epoch_losses = np.zeros((n_epochs, 2))\n",
    "    logger.info(f\"Iterations per epoch: {t_cfg.train_size * 1. / t_cfg.batch_size:3.3f} ~ {iter_per_epoch:d}.\")\n",
    "    \n",
    "    # Create batch generator\n",
    "    batch_gen = train_data.generate_batch(t_cfg.batch_size)\n",
    "    \n",
    "    n_iter = 0\n",
    "\n",
    "    for e_i in range(n_epochs):\n",
    "        \n",
    "        # Loop through the data with steps of batch_size\n",
    "        for t_i in range(0, t_cfg.train_size, t_cfg.batch_size):\n",
    "            \n",
    "            # Get the data for the set of batches\n",
    "            feats, y_history, y_target = next(batch_gen)\n",
    "            \n",
    "            # Train one iteration and get the loss returned\n",
    "            loss = train_iteration(net, t_cfg.loss_func, feats, y_history, y_target)\n",
    "\n",
    "            # Save the loss\n",
    "            iter_losses[e_i * iter_per_epoch + t_i // t_cfg.batch_size] = loss\n",
    "            # if (j / t_cfg.batch_size) % 50 == 0:\n",
    "            #    self.logger.info(\"Epoch %d, Batch %d: loss = %3.3f.\", i, j / t_cfg.batch_size, loss)\n",
    "            net.enc_lr.step()\n",
    "            net.dec_lr.step()\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "\n",
    "        y_test_pred = predict(net, train_data,\n",
    "                                t_cfg.train_size, t_cfg.batch_size, t_cfg.T,\n",
    "                                on_train=False)\n",
    "            \n",
    "        y_train_pred = predict(net, train_data,\n",
    "                                t_cfg.train_size, t_cfg.batch_size, t_cfg.T,\n",
    "                                on_train=True)\n",
    "            \n",
    "        val_loss = y_test_pred - train_data.data.targs[t_cfg.train_size:]\n",
    "        epoch_losses[e_i, :] = [np.mean(iter_losses[range(e_i * iter_per_epoch, (e_i + 1) * iter_per_epoch)]),\n",
    "                               np.mean(np.abs(val_loss))]\n",
    "        logger.info(f\"Epoch {(e_i+1):d}, train loss: {epoch_losses[e_i, 0]:3.6f}, val loss: {epoch_losses[e_i, 1]:3.6f}.\")\n",
    "\n",
    "        # Print som intermediate result every 5 epochs\n",
    "        if (e_i + 1) % 5 == 0:\n",
    "            \n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(range(1, 1 + len(train_data.data.targs)), train_data.data.targs,\n",
    "                     label=\"True\")\n",
    "            plt.plot(range(t_cfg.T, len(y_train_pred) + t_cfg.T), y_train_pred,\n",
    "                     label='Predicted - Train')\n",
    "            plt.plot(range(t_cfg.T + len(y_train_pred), len(train_data.data.targs) + 1), y_test_pred,\n",
    "                     label='Predicted - Test')\n",
    "            plt.legend(loc='upper left')\n",
    "            utils.save_or_show_plot(f\"pred_{e_i}.png\", save_plots)\n",
    "\n",
    "    return iter_losses, epoch_losses\n",
    "\n",
    "\n",
    "def train_iteration(t_net: DaRnnNet, loss_func: typing.Callable, X, y_history, y_target):\n",
    "    # Perform one training iteration for one batch\n",
    "    \n",
    "    # Zero the gradients\n",
    "    t_net.enc_opt.zero_grad()\n",
    "    t_net.dec_opt.zero_grad()\n",
    "\n",
    "    # Forward through the encoder and decorder\n",
    "    input_weighted, input_encoded = t_net.encoder(numpy_to_tvar(X))\n",
    "    y_pred = t_net.decoder(input_encoded, numpy_to_tvar(y_history))\n",
    "\n",
    "    # Calculate loss\n",
    "    y_true = numpy_to_tvar(y_target)\n",
    "    loss = loss_func(y_pred, y_true)\n",
    "    \n",
    "    # Take one step\n",
    "    loss.backward()\n",
    "\n",
    "    t_net.enc_opt.step()\n",
    "    t_net.dec_opt.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def predict(t_net: DaRnnNet, t_dat: TrainData, train_size: int, batch_size: int, T: int, on_train=False):\n",
    "    out_size = t_dat.data.targs.shape[1]\n",
    "    if on_train:\n",
    "        y_pred = np.zeros((train_size - T + 1, out_size))\n",
    "    else:\n",
    "        y_pred = np.zeros((t_dat.data.feats.shape[0] - train_size, out_size))\n",
    "\n",
    "    # Loop through the y_pred vector in batches\n",
    "    for y_i in range(0, len(y_pred), batch_size):\n",
    "        # Create a slice in the y_pred tensor\n",
    "        y_slc = slice(y_i, y_i + batch_size)\n",
    "        # Get the corresponding incides for the slice\n",
    "        batch_idx = range(len(y_pred))[y_slc]\n",
    "        # Same as batch size?\n",
    "        b_len = len(batch_idx)\n",
    "        # X with size (batch size) x (time window) x (number of features)\n",
    "        X = np.zeros((b_len, T, t_dat.data.feats.shape[1]))\n",
    "        # y_history with size (batch size) x (time windows - 1) x (number of targets)\n",
    "        y_history = np.zeros((b_len, T - 1, t_dat.data.targs.shape[1]))\n",
    "\n",
    "        # Loop through each sample in the batch\n",
    "        for b_i, b_idx in enumerate(batch_idx):\n",
    "            # Get the indices for the data to get\n",
    "            if on_train:\n",
    "                idx_x = range(b_idx, b_idx + T )\n",
    "                idx_yhist = range(b_idx, b_idx + T - 1)\n",
    "            else:\n",
    "                idx_x = range(b_idx + train_size - T, b_idx + train_size)\n",
    "                idx_yhist = range(b_idx + train_size - T, b_idx + train_size - 1)\n",
    "\n",
    "            X[b_i, :, :] = t_dat.data.feats[idx_x, :]\n",
    "            y_history[b_i, :] = t_dat.data.targs[idx_yhist]\n",
    "\n",
    "        y_history = numpy_to_tvar(y_history)\n",
    "        _, input_encoded = t_net.encoder(numpy_to_tvar(X))\n",
    "        y_pred[y_slc] = t_net.decoder(input_encoded, y_history).cpu().data.numpy()\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def predict_df(df, features, target_cols, t_net: DaRnnNet, batch_size: int, T: int, date_start, date_end):\n",
    "    \n",
    "    #date_start = df.index[0]\n",
    "    #date_end = df.index[-1]\n",
    "    t_dat = MyDataset(df, features, target_cols, T, date_start, date_end)\n",
    "    y_scaler = t_dat.y_scaler\n",
    "    \n",
    "    out_size = t_dat.data.targs.shape[1]\n",
    "    y_pred = np.zeros((t_dat.data.index.shape[0], out_size))\n",
    "    \n",
    "    # Loop through the y_pred vector in batches\n",
    "    for y_i in range(T, len(y_pred), batch_size):\n",
    "        \n",
    "        # Create a slice in the y_pred tensor\n",
    "        y_slc = slice(y_i, y_i + batch_size)\n",
    "        \n",
    "        # Get the corresponding incides for the slice\n",
    "        batch_idx = range(len(y_pred))[y_slc]\n",
    "        \n",
    "        # Same as batch size?\n",
    "        b_len = len(batch_idx)\n",
    "\n",
    "        # X with size (batch size) x (time window) x (number of features)\n",
    "        X = np.zeros((b_len, T, t_dat.data.feats.shape[1]))\n",
    "        \n",
    "        # y_history with size (batch size) x (time windows - 1) x (number of targets)\n",
    "        y_history = np.zeros((b_len, T - 1, t_dat.data.targs.shape[1]))\n",
    "\n",
    "        # Loop through each sample in the batch\n",
    "        for b_i, b_idx in enumerate(batch_idx):\n",
    "            \n",
    "            # Get the indices for the data to get\n",
    "            idx_x = range(b_idx - T, b_idx)\n",
    "            idx_yhist = range(b_idx - T, b_idx - 1)\n",
    "\n",
    "            X[b_i, :, :] = t_dat.data.feats[idx_x, :]\n",
    "            y_history[b_i, :] = t_dat.data.targs[idx_yhist]\n",
    "\n",
    "        y_history = numpy_to_tvar(y_history)\n",
    "        _, input_encoded = t_net.encoder(numpy_to_tvar(X))\n",
    "        y_pred[y_slc] = t_net.decoder(input_encoded, y_history).cpu().data.numpy()\n",
    "        \n",
    "    new_cols = [col+'_pred' for col in target_cols]\n",
    "    y_pred_inv = y_scaler.inverse_transform(y_pred)\n",
    "    df_y_pred = pd.DataFrame(data=y_pred_inv, index=t_dat.data.index, columns=new_cols)\n",
    "    \n",
    "    df = pd.merge(df, df_y_pred, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plots = True\n",
    "debug = False\n",
    "\n",
    "target_cols = [\n",
    "    'col1'\n",
    "    ]\n",
    "features = [\n",
    "    'col2'\n",
    "]\n",
    "\n",
    "train_start = datetime(2013, 1, 1)\n",
    "train_end = datetime(2014, 12, 31)\n",
    "test_start = datetime(2016, 1, 1)\n",
    "test_end = datetime(2016, 6, 13)\n",
    "\n",
    "data_train = MyDataset(df, features, target_cols, 12, train_start, train_end)\n",
    "x_scaler_train = data_train.x_scaler\n",
    "y_scaler_train = data_train.y_scaler\n",
    "data_test = MyDataset(df, features, target_cols, 12, test_start, test_end)\n",
    "x_scaler_test = data_test.x_scaler\n",
    "y_scaler_test = data_test.y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import Encoder, Decoder\n",
    "\n",
    "da_rnn_kwargs = {\"batch_size\": 128, \"T\": 12, \"encoder_hidden_size\": 128, \"decoder_hidden_size\": 128}\n",
    "config, model = da_rnn(data_train, **da_rnn_kwargs)\n",
    "iter_loss, epoch_loss = train(model, data_train, config, n_epochs=30, save_plots=save_plots)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.semilogy(range(len(iter_loss)), iter_loss)\n",
    "utils.save_or_show_plot(\"iter_loss.png\", save_plots)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.semilogy(range(epoch_loss.shape[0]), epoch_loss[:, 0], label='Train loss')\n",
    "plt.semilogy(range(epoch_loss.shape[0]), epoch_loss[:, 1], label='Test loss')\n",
    "plt.legend(loc='upper right')\n",
    "utils.save_or_show_plot(\"epoch_loss.png\", save_plots)\n",
    "\n",
    "final_y_pred_train = predict(model, data_train, config.train_size, config.batch_size, config.T, on_train=True)\n",
    "final_y_pred_test = predict(model, data_train, config.train_size, config.batch_size, config.T, on_train=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 1 + len(data_train.data.targs)), data_train.data.targs, label=\"True\")\n",
    "plt.plot(range(config.T, len(final_y_pred_train) + config.T), final_y_pred_train, label='Predicted - Train')\n",
    "plt.plot(range(config.T + len(final_y_pred_train), len(data_train.data.targs) + 1), final_y_pred_test, label='Predicted - Test')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "from bokeh.plotting import output_notebook; output_notebook(notebook_type='jupyter') \n",
    "\n",
    "# Calculate maximal error in train data\n",
    "df_train_preds = predict_df(df, features, target_cols, model, config.batch_size, config.T, train_start, train_end)\n",
    "df_train_preds = df_train_preds[train_start:train_end].dropna()\n",
    "\n",
    "max_errors = {}\n",
    "for target_col in target_cols: \n",
    "    df_train_preds[target_col+'_error'] = df_train_preds[target_col] - df_train_preds[target_col+'_pred']\n",
    "    max_error = np.percentile(np.abs(df_train_preds[target_col+'_error']), 99.5)\n",
    "    max_errors[target_col] = max_error\n",
    "\n",
    "# Make prediction for a new time span\n",
    "date_start = datetime(2014, 1, 1)\n",
    "date_end = datetime(2016, 6, 13)\n",
    "df_test = predict_df(df, features, target_cols, model, config.batch_size, config.T, date_start, date_end)\n",
    "df_test = df_test[date_start:date_end]\n",
    "\n",
    "for target_col in target_cols: \n",
    "    df_test[target_col+'_error'] = df_test[target_col] - df_test[target_col+'_pred']\n",
    "    df_test[target_col+'_anomaly'] = 0\n",
    "    df_test.loc[np.abs(df_test[target_col+'_error']) >= max_errors[target_col], target_col+'_anomaly'] = 1\n",
    "    df_test[target_col+'_cum_anomaly'] = df_test[target_col+'_anomaly'].cumsum()\n",
    "\n",
    "\n",
    "\n",
    "p = bokeh.plotting.figure(title='test', x_axis_label='x', y_axis_label='y', plot_width=1400, plot_height=1000, y_range=(0, 70), x_axis_type=\"datetime\")\n",
    "p.extra_y_ranges = {'foo': Range1d(start=0, end=1000)}\n",
    "p.add_layout(LinearAxis(y_range_name=\"foo\"), 'right')\n",
    "for target_col in target_cols:\n",
    "    p.line(df_test.index, df_test[target_col], legend=\"True\", line_width=1, line_color='blue')\n",
    "    p.line(df_test.index, df_test[target_col+'_pred'], legend=\"Predicted\", line_width=1, line_color='red')\n",
    "    p.line(df_test.index, df_test[target_col+'_cum_anomaly'], legend=\"Cum. Anomaly\", line_width=1, line_color='green', y_range_name='foo')\n",
    "\n",
    "bokeh.plotting.show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
